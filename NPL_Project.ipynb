{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLYW5HHD-K5B"
      },
      "source": [
        "# Fine-tuning FLAN-T5-small for Mythology QA with PEFT on Low VRAM\n",
        "\n",
        "This notebook demonstrates how to fine-tune `google/flan-t5-small` for a question-answering task using your mythology dataset, specifically optimized for low VRAM environments (like 4GB) using PEFT (LoRA) and 8-bit quantization. It includes handling for train, validation, and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LSxBBnb-Qbs"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kut9NQJ97CmU",
        "outputId": "1fbdc0ca-6d18-4bc0-9465-1c5561d94768"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets peft bitsandbytes accelerate torch tensorboard -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpAxf3EjFS1R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JucjhPKo-e2V"
      },
      "source": [
        "## 2. Configuration\n",
        "Adjust these parameters based on your setup and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLBzE-qhGltj"
      },
      "outputs": [],
      "source": [
        "# --- Model Configuration ---\n",
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "\n",
        "SINGLE_DATA_FILE = \"/content/temp.jsonl\"\n",
        "\n",
        "# --- Output Directories ---\n",
        "OUTPUT_DIR = \"./flan-t5-base-mythology-results\" # Checkpoints and logs\n",
        "PEFT_ADAPTER_DIR = \"./flan-t5-base-mythology-peft-adapter\" # Final adapter\n",
        "\n",
        "# --- Training Hyperparameters (CRUCIAL TUNING FOR 4GB VRAM) ---\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 1 # Keep 1 or 2 for 4GB VRAM\n",
        "GRAD_ACCUMULATION_STEPS = 16 # Increase this (e.g., 16, 32, 64) to compensate for small BATCH_SIZE\n",
        "NUM_EPOCHS = 3 # Adjust as needed based on validation performance\n",
        "MAX_SEQ_LENGTH = 512 # Max token length for input/output. Reduce if OOM errors occur.\n",
        "\n",
        "# --- PEFT/LoRA Configuration ---\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q\", \"v\"] # For FLAN-T5 Query/Value projection layers\n",
        "\n",
        "# --- Quantization Configuration ---\n",
        "USE_8BIT = True # Set to True for 8-bit, False for 4-bit\n",
        "USE_4BIT = False # Set to True for 4-bit\n",
        "\n",
        "if USE_8BIT:\n",
        "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "elif USE_4BIT:\n",
        "     bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "else:\n",
        "    bnb_config = None\n",
        "\n",
        "print(f\"Using Quantization: {'8-bit' if USE_8BIT else ('4-bit' if USE_4BIT else 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndt1HquG-0f8"
      },
      "source": [
        "## 3. Load Dataset\n",
        "This section loads the dataset. It assumes you have `train`, `validation`, and `test` splits available (e.g., as separate files in `DATASET_PATH`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUQaqdP0-tJL"
      },
      "outputs": [],
      "source": [
        "print(f\"Attempting to load single file {SINGLE_DATA_FILE} and split...\")\n",
        "full_dataset = load_dataset(\"json\", data_files=SINGLE_DATA_FILE, split='train')\n",
        "# Split: 80% train, 20% validation\n",
        "train_testvalid = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'validation': train_testvalid['test'],\n",
        "})\n",
        "print(\"Dataset loaded from single file and split:\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTGwlW_7_ATr"
      },
      "source": [
        "## 4. Preprocessing & Filtering\n",
        "Format the input, tokenize, AND filter out examples exceeding max length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIGwwCuf_G6r"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer (if not already loaded)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syDc7TPW_Kto"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_check_length(examples):\n",
        "    \"\"\"\n",
        "    Formats input/output, tokenizes to check length,\n",
        "    and tokenizes again with padding/truncation for model input.\n",
        "    \"\"\"\n",
        "    # --- Format Inputs and Targets ---\n",
        "    inputs = [f\"question: {q} context: {c}\" for q, c in zip(examples['question'], examples['context'])]\n",
        "    targets = [str(ans) for ans in examples['answer']] # Ensure answers are strings\n",
        "\n",
        "    # --- Tokenize WITHOUT truncation/padding to check actual lengths ---\n",
        "    # This gives us the true token count before any modification\n",
        "    input_tokens = tokenizer(inputs)\n",
        "    target_tokens = tokenizer(targets)\n",
        "    examples['input_length'] = [len(ids) for ids in input_tokens['input_ids']]\n",
        "    examples['target_length'] = [len(ids) for ids in target_tokens['input_ids']]\n",
        "\n",
        "    # --- Tokenize WITH padding/truncation for model ---\n",
        "    # This prepares the data that will actually be fed into the model\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", truncation=True)\n",
        "    labels = tokenizer(targets, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # --- Prepare Labels for Loss Calculation ---\n",
        "    # Set -100 for padding tokens in labels so they are ignored in loss.\n",
        "    processed_labels = []\n",
        "    for label_ids in labels[\"input_ids\"]:\n",
        "         processed_labels.append([label_id if label_id != tokenizer.pad_token_id else -100 for label_id in label_ids])\n",
        "\n",
        "    # Add the processed model inputs and labels to the examples dictionary\n",
        "    examples['input_ids'] = model_inputs['input_ids']\n",
        "    examples['attention_mask'] = model_inputs['attention_mask']\n",
        "    examples['labels'] = processed_labels\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMhba6qb_OB6"
      },
      "outputs": [],
      "source": [
        "print(\"Applying preprocessing and length checking...\")\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_and_check_length,\n",
        "    batched=True,\n",
        "    \n",
        ")\n",
        "print(\"Preprocessing and length checking complete.\")\n",
        "print(\"Dataset structure after length check:\")\n",
        "print(processed_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mybHCDfW_R8z"
      },
      "source": [
        "### Filtering Step\n",
        "Now, remove examples where the original input or target length exceeds the limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BNSXX70_X_1"
      },
      "outputs": [],
      "source": [
        "def filter_long_examples(example):\n",
        "    \"\"\"Returns True if both input and target lengths are within limits.\"\"\"\n",
        "    return example['input_length'] <= MAX_SEQ_LENGTH and example['target_length'] <= MAX_SEQ_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0sEUchw_bQ_"
      },
      "outputs": [],
      "source": [
        "print(\"Filtering out examples exceeding max length...\")\n",
        "original_sizes = {split: len(processed_datasets[split]) for split in processed_datasets.keys()}\n",
        "\n",
        "# Apply the filter function to all splits in the DatasetDict\n",
        "filtered_datasets = processed_datasets.filter(filter_long_examples, batched=True) # Batched filtering is faster\n",
        "\n",
        "filtered_sizes = {split: len(filtered_datasets[split]) for split in filtered_datasets.keys()}\n",
        "print(\"\\nFiltering complete.\")\n",
        "\n",
        "# Print comparison\n",
        "print(\"\\nDataset sizes before filtering:\")\n",
        "print(original_sizes)\n",
        "print(\"Dataset sizes after filtering:\")\n",
        "print(filtered_sizes)\n",
        "\n",
        "\n",
        "# Final dataset to be used for training\n",
        "tokenized_datasets = filtered_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IIx_A_U_ptQ"
      },
      "source": [
        "## 5. Load Model, Apply Quantization & PEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4YOnsTb_guI"
      },
      "outputs": [],
      "source": [
        "print(\"Loading base model...\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Automatically distributes model across available devices (GPU prioritized)\n",
        ")\n",
        "print(\"Base model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFFUIxx8_tyB"
      },
      "outputs": [],
      "source": [
        "if bnb_config:\n",
        "    print(\"Preparing model for K-bit training (if quantized)...\")\n",
        "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # Enable gradient checkpointing here\n",
        "    print(\"Model prepared for K-bit training.\")\n",
        "else:\n",
        "     # Still enable gradient checkpointing if not quantizing but wanting memory savings\n",
        "    model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICevxRSU_wjp"
      },
      "outputs": [],
      "source": [
        "print(\"Configuring LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "print(\"Applying LoRA adapter...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA applied.\")\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPUyZhpF_0mx"
      },
      "source": [
        "## 6. Configure Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDd2lBKR_3Nl"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
        "    gradient_checkpointing=True, # Already enabled via prepare_model_for_kbit_training or directly\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10, # Log training loss every 10 steps\n",
        "    evaluation_strategy=\"epoch\", # Evaluate performance every epoch using validation set\n",
        "    save_strategy=\"epoch\",       # Save model checkpoint every epoch\n",
        "    save_total_limit=2,          # Keep only the last 2 checkpoints\n",
        "    load_best_model_at_end=True, # Load the best model checkpoint (based on validation loss) at the end of training\n",
        "    metric_for_best_model=\"eval_loss\", # Use validation loss to determine the best model\n",
        "    greater_is_better=False,     # Lower validation loss is better\n",
        "    fp16=True,                   # Enable mixed precision training (can speed up and save some memory)\n",
        "    optim=\"adamw_bnb_8bit\" if USE_8BIT or USE_4BIT else \"adamw_torch\", # Memory efficient optimizer if quantized\n",
        "    report_to=\"tensorboard\",     # Log metrics for TensorBoard visualization\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBge3iGM__xP"
      },
      "source": [
        "## 7. Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq-F-oSPACHc"
      },
      "outputs": [],
      "source": [
        "# Data Collator for Seq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100 # Pad labels with -100 to ignore in loss calculation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2HO9NMIAE2B"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"], # Provide validation dataset\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Optional: Stop if eval_loss doesn't improve for 3 epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN1bVXZ0AHi-"
      },
      "source": [
        "## 8. Train the Model\n",
        "This will take time, depending on dataset size and hardware. Monitor VRAM usage!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On0C_DiDALqx"
      },
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Log training metrics\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZkmePaAadf"
      },
      "source": [
        "## 9. Save Final PEFT Adapter\n",
        "This saves only the trained adapter weights, which are much smaller than the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H07_JP1iAe2A"
      },
      "outputs": [],
      "source": [
        "print(f\"Saving the final PEFT adapter to {PEFT_ADAPTER_DIR}...\")\n",
        "# Use save_model to save the adapter and tokenizer correctly with PEFT\n",
        "trainer.save_model(PEFT_ADAPTER_DIR)\n",
        "\n",
        "print(\"Adapter saved.\")\n",
        "print(\"Fine-tuning process complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCVC7b00Ahr-"
      },
      "source": [
        "## 10. Loading the Saved Adapter (Example for Inference Later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtO5EUWLAkyb"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "print(\"Loading adapter for inference...\")\n",
        "config = PeftConfig.from_pretrained(PEFT_ADAPTER_DIR)\n",
        "\n",
        "# Load the base model again with quantization config\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    quantization_config=bnb_config, # Use the same quantization\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# Load the PEFT model\n",
        "inference_model = PeftModel.from_pretrained(base_model, PEFT_ADAPTER_DIR)\n",
        "inference_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "print(\"Inference model loaded.\")\n",
        "\n",
        "# Example Inference\n",
        "context = \"Your mythology context here...\"\n",
        "question = \"Your question here...\"\n",
        "input_text = f\"question: {question} context: {context}\"\n",
        "inputs = inference_tokenizer(input_text, return_tensors=\"pt\").to(inference_model.device) # Ensure tensors are on the same device\n",
        "\n",
        "outputs = inference_model.generate(**inputs, max_new_tokens=50)\n",
        "answer = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nExample Inference Answer: {answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
